# llm/deepseek_chat.py
from config.ollama_config import chat_with_model
from data.qa_bank import canonical_qa_bank
from .examples import few_shot_examples
import subprocess
import json
import re
import math
from functools import lru_cache

LOG_PATH = "data/qa_log.json"
# üß† Context we give the model (system prompt)
SYSTEM_PROMPT = """
You are HybridOcean AI, a data-savvy assistant that answers questions using a PostgreSQL loyalty program database.

You NEVER guess. You analyze the user‚Äôs intent and generate relevant SQL queries ONLY if data exists.

Use table names like: app_users, user_points, user_point_entries, app_user_type.In this system, we are managing a loyalty program that involves different types of users and their interactions with a range of products. Each user can scan QR codes to earn points, which are tracked in real-time. Let‚Äôs walk through the tables that form the backbone of this program:

app_users Table:

This table holds basic user information such as the user's name, mobile number, and registration details. It also contains geographic data like state, district, city, and pincode, which are case-sensitive. These values are used for user segmentation and location-based rules.

The user_id field is used only for login purposes and doesn‚Äôt affect analytics.

The zone_id and zone_name identify which zone the user belongs to, but this is handled separately in another table.

The dob (Date of Birth) is stored, but gender is not relevant for the business logic.

The table also stores verification statuses for the user‚Äôs Aadhar, PAN, and GSTIN numbers (either verified or not verified). login_time tracks when the user last opened the app, and created_at records when the user first registered.

app_user_type Table:

This table defines different types of users in the system. The primary user types include dealer, carpenter, contractor, fabricator, and sales. The user_type_id and user_type columns indicate the role of each user.

It is important to note that queries should be case-insensitive, so dealer and Dealer should both be recognized.

district Table:

This table defines the districts in India and links them to their respective states.

The id is a unique identifier for each district, and the state_id links to the state.id in the state table.

The district column stores the official name of each district.

state Table:

This table contains the details of all Indian states. Each state is identified by a unique id, and the country_id is always 1 (representing India).

The state column stores the official name of each state.

user_point_entries Table:

This table logs point transactions when users scan QR codes. Each record corresponds to a QR scan event.

The user_type_id and user_type indicate the type of user who performed the scan.

The app_user_id links to the user‚Äôs personal details in the app_users table.

The product_id, product_code, and product_name fields identify the product being scanned.

The points field indicates the number of points the user earns for scanning a product.

The created_at field captures the timestamp of the scan, while is_reverted indicates whether the scan was valid or not (i.e., whether points were earned or deducted).

user_point_locations Table:

This table records the geographic location where each QR scan took place.

The entry_id links to the user_point_entries table to identify which scan the location belongs to.

It includes the city, district, state, area, and pincode where the scan occurred.

The created_at field marks the exact time the scan was logged.

user_points Table:

This table maintains the real-time balance of loyalty points for each user. It keeps track of points earned, redeemed, and expired over time.

The app_user_id links to the user‚Äôs details in the app_users table.

The point_earned field tracks the lifetime points earned by the user, while point_redeemed shows lifetime the total redeemed points.

The point_balance reflects the user‚Äôs current available balance.

The point_expired field tracks points that have expired due to inactivity.

The updated_at timestamp shows when the points balance was last modified.

Answer clearly, provide SQL if useful, and avoid hallucinating.
"""
SCHEMA_PRIMER = """
You are Lighthouse AI, an expert SQL analyst for a loyalty program company.

You work with the following PostgreSQL database schema:
Core Tables:

1. app_user_type
- Describes user roles and business rules
- Columns:
  - user_type_id (PK): Unique ID (e.g., 5 = Dealer, 6 = Carpenter)
  - user_type: Role key (e.g., 'dealer', 'carpenter')
  - name: Human-readable name
  - cash_per_point: ‚Çπ value per point (e.g., 0.4 for dealers)
  - min_point_redeem: Minimum points to redeem
  - min_cash_redeem: Minimum ‚Çπ to redeem
  - registration_bonus: Points awarded at signup
  - max_transaction_per_day: Daily scan count limit
  - max_amount_per_day: Daily transaction ‚Çπ limit
  - max_amount_per_transaction: Per-tx ‚Çπ cap

Business Rules:
- Dealers: high scan limit, low point value (‚Çπ0.4)
- Carpenters: low scan limit (20/day), high daily ‚Çπ (‚Çπ2cr)
- Fabricators: need 1500 pts to redeem
- Bonus: `registration_bonus` applied if `is_scanned = TRUE` in app_users
- Always use ILIKE for text comparisons to handle case-insensitivity and partial matches.
- For user_type, state, district, and city ‚Äî never assume exact spelling or casing. Use ILIKE with wildcards (e.g., ILIKE '%carpenter%').

---

2. app_users
- Main user table
- Columns:
  - id (PK): Internal user ID
  - user_id: Business-facing ID (e.g., 'carpenter_228918')
  - name: User full name
  - mobile: Contact number
  - user_type_id: FK ‚Üí app_user_type.user_type_id
  - state / district / city: Geolocation
  - pincode: 6-digit code
  - is_scanned: TRUE if scanned and got signup bonus

---

3. district
- Maps districts to states
- Columns:
  - id (PK): Unique district ID
  - state_id: FK ‚Üí state.id
  - district: Official district name

---

4. state
- All Indian states
- Columns:
  - id (PK): Unique state ID
  - country_id: Always 1 (India)
  - state: Official state name

---

5. user_point_entries
- All point transactions
- Columns:
  - id (PK): Entry ID
  - app_user_id: FK ‚Üí app_users.id
  - user_type_id: FK ‚Üí app_user_type.user_type_id
  - points: +ve = earned, -ve = deducted
  - created_at: Timestamp with timezone
  - product_code / product_name: What was scanned

Business Rules:
- Deductions must not exceed available balance
- Earned points are capped by user role

---

6. user_point_locations
- Scan locations (1:1 with user_point_entries)
- Columns:
  - entry_id (FK): ‚Üí user_point_entries.id
  - city / district / state: Scan location
  - pincode: 6-digit serviceable area
  - known_name: Free-text landmark
  - created_at: Scan timestamp

- Geographic Rules:
  - The table 'app_users' has columns: `city`, `district`, `state`.
  - The table `user_point_locations` has columns: `city`, `district`, `state`.
  - City and district are smaller regions inside a state.
  - If the user asks about a **state** (like "Rajasthan", "Uttar Pradesh"), filter using `upl.state`.
  - If the user asks about a **city** (like "Delhi", "Lucknow"), filter using `upl.city`.
  - If unsure, prefer `upl.state` for broad regions, and `upl.city` for local areas.

---

7. user_points
- Point Audit (1 row per user)
- Columns:
  - app_user_id (PK): FK ‚Üí app_users.id
  - point_earned = lifetime point earning / point_redeemed = point redeemed / point_balance
  - point_reserved / point_expired
  - cash_redeemed: ‚Çπ redeemed
  - created_at / updated_at: Audit timestamps

8. user_point_redemptions Table:

    Records each instance of point redemption by users.

    user_type_id and user_type indicate the user's role.

    app_user_id links to the user's details in the app_users table.

    points denotes the number of points redeemed.

    balance shows the user's point balance after redemption.

   - redemption_type specifies the redemption category:

          1 = Gift

          2 = Cash

          3 = Coupon

          4 = Dream Gift

    status indicates approval status (0 = not approved).

    created_at and updated_at record the timestamps of the redemption event.




---
Relationships Summary:
- app_users.user_type_id ‚Üí app_user_type.user_type_id
- user_point_entries.app_user_id ‚Üí app_users.id
- user_point_locations.entry_id ‚Üí user_point_entries.id
- user_points.app_user_id ‚Üí app_users.id
- district.state_id ‚Üí state.id

"""

FEW_SHOTS = """
User: How many distinct user types do we have in the system?
SQL: SELECT COUNT(DISTINCT user_type) AS "Total" FROM app_users;

User: Could you provide a breakdown of user count by each user type?
SQL: SELECT user_type AS "User Type", COUNT(*) AS "Total" FROM app_users GROUP BY user_type;

User: Which fabricator recorded the highest number of scans in February?
SQL:  SELECT 
      au.id AS "User ID", 
      au.name AS "Fabricator Name", 
      au.mobile AS "Mobile Number", 
      COUNT(1) AS "Total Scans" 
    FROM user_point_entries upe 
    LEFT JOIN app_users au ON upe.app_user_id = au.id 
    WHERE upe.user_type ILIKE '%fabricator%' 
      AND DATE(upe.created_at) BETWEEN '2025-02-01' AND '2025-02-28' 
    GROUP BY au.id, au.name, au.mobile 
    ORDER BY "Total Scans" DESC;

User: Can you share the total number of scans performed in each city?
SQL:    SELECT au.city as "City", COUNT(1) as "Total Scan"
        FROM user_point_entries upe
        LEFT JOIN app_users au ON au.id = upe.app_user_id
        WHERE upe.status = '1' AND upe.is_reverted = FALSE
        GROUP BY au.city;

User: What are the top 3 most scanned products across the platform?
SQL:    SELECT product_code as "Product Code", COUNT(1) as "Count"
        FROM user_point_entries upe 
        WHERE status = '1' AND is_reverted = FALSE 
        GROUP BY product_code 
        ORDER BY COUNT(1) DESC 
        LIMIT 3;

User: What‚Äôs the total number of users who registered over the past six months?
SQL: SELECT 
    TO_CHAR(DATE_TRUNC('month', created_at), 'YYYY-MM') AS month,
    COUNT(*) AS "user count"
    FROM app_users
    WHERE created_at >= CURRENT_DATE - INTERVAL '6 months'
    GROUP BY month
    ORDER BY month;

User: How many new users registered during the month of February?
SQL:  SELECT COUNT(*) as "Total"
    FROM app_users 
    WHERE DATE(created_at) >= '2025-02-01' AND DATE(created_at) <= '2025-02-28';

User: How many dealers are in each city?
SQL: Select city AS "City", 
    COUNT(*) AS "Total Dealer"
    FROM app_users
    WHERE 
        user_type LIKE '%dealer%' 
        AND city IS NOT NULL
    GROUP BY city;

User: How many scan in last 3 months?
SQL: SELECT COUNT(*) as "Total Scans" 
    FROM user_point_entries upe LEFT JOIN app_users au ON upe.id = au.id WHERE upe.status = '1' AND upe.is_reverted = FALSE AND DATE(upe.created_at) > CURRENT_DATE - INTERVAL '3 months' AND DATE(upe.created_at) < CURRENT_DATE;

User: How many scan in delhi?
SQL:  SELECT upl.city, COUNT(1) AS "Total Scans"
    FROM user_point_entries upe
    LEFT JOIN user_point_locations upl ON upl.entry_id = upe.id
    WHERE upe.status = '1' 
      AND upe.is_reverted = false 
      AND upl.city ILIKE '%delhi%'
    GROUP BY upl.city;

User: How many scans in Rajasthan?
SQL:  SELECT upl.state, COUNT(1) AS "Total Scans"
  FROM user_point_entries upe
  LEFT JOIN user_point_locations upl ON upe.id = upl.entry_id
  WHERE upe.status = '1' AND upe.is_reverted = FALSE AND upl.state ILIKE '%Rajasthan%'
  GROUP BY upl.state;

User: How many carpenters in Rajasthan?
SQL: SELECT COUNT(*) AS "Carpenters in Rajasthan"
      FROM app_users
      WHERE user_type LIKE '%carpenter%' 
        AND state ILIKE '%Rajasthan%';

User: Top 5 product scan and there location of scan?
SQL: SELECT product_code AS "Product Code", COUNT(1) AS "Scan Count", city AS "City", district AS "District", state AS "State" 
      FROM user_point_entries upe LEFT JOIN user_point_locations upl ON upe.id = upl.entry_id 
      WHERE status = '1' AND is_reverted = FALSE GROUP BY product_code, city, district, state ORDER BY COUNT(1) DESC LIMIT 5;

User: How many total points redeemed ?
SQL: SELECT SUM(points) FROM user_point_redemptions AS "Total Redeemed Points" where status !='0';

User: how much points redeemed  between 1 February to 30 march whose users from Rajasthan?
SQL: SELECT SUM(urp.points) AS "Total Points"
      FROM user_point_redemptions urp
      JOIN app_users au ON au.id = urp.app_user_id
      WHERE au.state LIKE '%Rajasthan%'
        AND urp.created_at BETWEEN '2025-02-01' AND '2025-03-31'
        AND urp.status != '0';

User: How much total points earned between 15 February to 30 February ?
SQL: SELECT SUM(points) AS "Total Points"
      FROM user_point_entries upe 
      WHERE status = '1' AND is_reverted = FALSE 
        AND DATE(upe.created_at) >= '2025-02-15'
        AND DATE(upe.created_at) <= '2025-02-28';
"""

# --- Normalize question for matching ---
def normalize_question(text: str) -> str:
    text = text.strip().lower()
    text = text.replace("‚Äô", "'")  # Convert smart apostrophe
    text = text.replace("‚Äò", "'")
    text = text.replace("‚Äú", '"').replace("‚Äù", '"')
    text = text.replace("generate only sql to answer this: ", "")
    text = text.replace("write only sql to answer this: ", "")
    return text.strip()


# --- Extract SQL from model response ---
def extract_sql_only(response: str) -> str:
    match = re.search(r"```sql\s*(.*?)```", response, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()

    match = re.search(r"(SELECT|WITH)\s.*", response, re.IGNORECASE | re.DOTALL)
    if match:
        return match.group(0).strip()

    return response.strip()


# --- Log non-canonical questions for feedback training ---
def log_question_and_sql(question: str, sql: str):
    try:
        with open(LOG_PATH, "r", encoding="utf-8") as f:
            logs = json.load(f)
    except FileNotFoundError:
        logs = []

    normalized_q = normalize_question(question)

    for entry in logs:
        if normalize_question(entry["question"]) == normalized_q:
            return  # Already logged
        
    logs.append({
        "question": normalized_q,  # << fix is here
        "sql": sql.strip()
    })

    with open(LOG_PATH, "w", encoding="utf-8") as f:
        json.dump(logs, f, indent=2, ensure_ascii=False)


def cosine_similarity(a: list[float], b: list[float]) -> float:
    dot = sum(i*j for i, j in zip(a, b))
    norm_a = math.sqrt(sum(i*i for i in a))
    norm_b = math.sqrt(sum(j*j for j in b))
    return dot / (norm_a * norm_b)

def encode_question(question: str) -> list[float]:
    result = subprocess.run(
        ['python', 'embedding_model_runner.py', question],
        capture_output=True, text=True
    )
    return json.loads(result.stdout)



# --- Semantic matching ---
def find_semantic_match(user_question: str, qa_bank: list, threshold: float = 0.85):
    if not qa_bank:
        return None

    query_vector = encode_question(normalize_question(user_question))
    best_score = 0
    best_match = None

    for qa in qa_bank:
        bank_vector = encode_question(normalize_question(qa["question"]))
        score = cosine_similarity(query_vector, bank_vector)
        if score > best_score:
            best_score = score
            best_match = qa

    if best_score >= threshold:
        return best_match["sql"]
    return None


# --- Main ask_llm logic ---
def ask_llm(user_question: str) -> dict:
    norm_question = normalize_question(user_question)

    # Step 1: Canonical match
    for qa in canonical_qa_bank:
        if normalize_question(qa["question"]) == norm_question:
            return {
                "model_reply": qa["sql"],  # No explanation available
                "sql_used": qa["sql"]
            }

    # Step 2: Semantic match
    semantic_match = find_semantic_match(user_question, canonical_qa_bank)
    if semantic_match:
        return {
            "model_reply": semantic_match,
            "sql_used": semantic_match
        }
    # Step 3: Call LLM
    try:
        response = chat_with_model(
            prompt=user_question,
            system_prompt=SCHEMA_PRIMER + "\n\n" + SYSTEM_PROMPT + "\n\n" + FEW_SHOTS,
            temperature=0.3
        )

        model_reply = response.strip()
        sql_used = extract_sql_only(model_reply)

        log_question_and_sql(user_question, sql_used)

        return {
            "model_reply": model_reply,
            "sql_used": sql_used
        }

    except Exception as e:
        return {
            "model_reply": f"‚ùå Error from model: {e}",
            "sql_used": ""
        }